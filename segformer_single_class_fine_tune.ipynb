{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "from datasets import load_from_disk, Dataset, ClassLabel\n",
    "from datasets import Image as ImageDS\n",
    "\n",
    "from transformers import SegformerFeatureExtractor\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from transformers import TrainingArguments\n",
    "from transformers import logging\n",
    "from transformers import Trainer\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# avoid name clash\n",
    "import copy as pcopy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from IPython.utils.io import capture_output\n",
    "\n",
    "import optuna\n",
    "\n",
    "# project library - datasets and dataloaders\n",
    "from bus_data import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_path = os.getcwd()\n",
    "\n",
    "train_dataset_path = \"huggingface_bus_ds_four_combo_single_class\"\n",
    "video_dataset_path = \"huggingface_bus_ds_video\"\n",
    "\n",
    "model_dir = \"segformer_single_class_model\"\n",
    "output_dir = \"segformer_single_class_outputs\"\n",
    "logging_dir = \"segformer_single_class_logs\"\n",
    "trials_dir = \"segformer_single_class_trials\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# output_dir is recreated every trial\n",
    "os.makedirs(logging_dir, exist_ok=True)\n",
    "os.makedirs(trials_dir, exist_ok=True)\n",
    "\n",
    "if os.uname()[1] == \"media\":\n",
    "    # Florin's gaming PC\n",
    "    data_volume = \"/home/florin/data\"\n",
    "else:\n",
    "    # Google Colab\n",
    "    from google.colab import drive\n",
    "\n",
    "    gdrive_path = \"/content/gdrive\"\n",
    "    drive.mount(gdrive_path, force_remount=False)\n",
    "    data_volume = gdrive_path + \"/MyDrive\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and/or load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(\n",
    "    {\n",
    "        \"dataset\": [],\n",
    "        \"image\": [],\n",
    "        \"mask\": [],\n",
    "        \"tumor\": [],\n",
    "    }\n",
    ")\n",
    "\n",
    "train_df = pd.concat([train_df, busis_dataset_make(data_volume)], ignore_index=True)\n",
    "train_df = pd.concat([train_df, bus_dataset_b_make(data_volume)], ignore_index=True)\n",
    "train_df = pd.concat(\n",
    "    [train_df, dataset_busi_with_gt_make(data_volume)], ignore_index=True\n",
    ")\n",
    "train_df = pd.concat([train_df, mayo_dataset_make(data_volume)], ignore_index=True)\n",
    "\n",
    "# convert Path to string, HuggingFace wants strings here\n",
    "train_df[\"image\"] = train_df[\"image\"].apply(lambda x: str(x))\n",
    "train_df[\"mask\"] = train_df[\"mask\"].apply(lambda x: [str(xi) for xi in x])\n",
    "train_df[\"dataset_tumor\"] = train_df[\"dataset\"] + \"_\" + train_df[\"tumor\"]\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping classes to/from pixel values\n",
    "id2label, label2id, num_labels = labels_ids_bus(multiclass=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_hf_train_ds(row):\n",
    "    \"\"\"\n",
    "    input: a Pandas DF row with image and labels\n",
    "    output: a dictionary with the image and the labels\n",
    "\n",
    "    Convert mask pixel values as needed:\n",
    "    - background = 0\n",
    "    - lesion = 1\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "\n",
    "    ret[\"dataset\"] = row[\"dataset\"]\n",
    "    ret[\"pixel_values\"] = Image.open(row[\"pixel_values\"]).convert(\"RGB\")\n",
    "\n",
    "    # merge all mask images into a single mask\n",
    "    mask_list = [\n",
    "        np.asarray(Image.open(x).convert(\"L\"), dtype=np.uint8) for x in row[\"mask\"]\n",
    "    ]\n",
    "\n",
    "    # set clip values based on mask existence\n",
    "    if row[\"tumor\"] == \"benign\" or row[\"tumor\"] == \"malignant\":\n",
    "        clip_val = label2id[\"lesion\"]\n",
    "    else:\n",
    "        clip_val = label2id[\"unlabeled\"]\n",
    "    # build the actual mask image\n",
    "    mask_frame = Image.fromarray(\n",
    "        np.clip(np.amax(np.stack(mask_list), axis=0), a_min=0, a_max=clip_val)\n",
    "    )\n",
    "    ret[\"label\"] = mask_frame\n",
    "\n",
    "    ret[\"tumor\"] = row[\"tumor\"]\n",
    "    ret[\"dataset_tumor\"] = row[\"dataset_tumor\"]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def create_hf_train_dataset(df):\n",
    "    \"\"\"\n",
    "    input: Pandas dataframe with the training dataset paths and labels\n",
    "    output: HuggingFace dataset with training data\n",
    "    \"\"\"\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"dataset\": df[\"dataset\"].to_list(),\n",
    "            \"pixel_values\": df[\"image\"].to_list(),\n",
    "            \"mask\": df[\"mask\"].to_list(),\n",
    "            \"tumor\": df[\"tumor\"].to_list(),\n",
    "            \"dataset_tumor\": df[\"dataset_tumor\"].to_list(),\n",
    "        }\n",
    "    )\n",
    "    dataset = dataset.map(map_hf_train_ds, num_proc=multiprocessing.cpu_count())\n",
    "    dataset = dataset.cast_column(\n",
    "        \"dataset_tumor\", ClassLabel(names=df[\"dataset_tumor\"].unique().tolist())\n",
    "    )\n",
    "    dataset = dataset.cast_column(\n",
    "        \"tumor\", ClassLabel(names=df[\"tumor\"].unique().tolist())\n",
    "    )\n",
    "    dataset = dataset.cast_column(\n",
    "        \"dataset\", ClassLabel(names=df[\"dataset\"].unique().tolist())\n",
    "    )\n",
    "    dataset = dataset.cast_column(\"pixel_values\", ImageDS())\n",
    "    return dataset.remove_columns(\"mask\")\n",
    "\n",
    "\n",
    "# If HuggingFace dataset exists, load it.\n",
    "# Otherwise create and save it.\n",
    "if os.path.isdir(train_dataset_path):\n",
    "    ds_hf = load_from_disk(train_dataset_path)\n",
    "else:\n",
    "    ds_hf = create_hf_train_dataset(train_df)\n",
    "    ds_hf.save_to_disk(train_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hf = ds_hf.shuffle(seed=1).train_test_split(\n",
    "    test_size=0.2, stratify_by_column=\"dataset_tumor\", seed=20\n",
    ")\n",
    "\n",
    "train_ds = ds_hf[\"train\"]\n",
    "test_ds = ds_hf[\"test\"]\n",
    "\n",
    "# unprocessed images to visualize results at the end\n",
    "# these don't have any augmentations or normalization\n",
    "# TODO: eliminate the need for this DS\n",
    "test_ds_orig = pcopy.deepcopy(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_palette(mpl_format=False):\n",
    "    \"\"\"\n",
    "    output: pixel values for classes in predictions\n",
    "\n",
    "    Contains values for the unlabeled background,\n",
    "    and also for the labeled objects.\n",
    "    \"\"\"\n",
    "    colors_list = [\n",
    "        [0, 0, 0],\n",
    "        [0, 255, 0],\n",
    "        [255, 0, 0],\n",
    "    ]\n",
    "    if mpl_format:\n",
    "        colors_list = [tuple([ch / 255.0 for ch in color]) for color in colors_list]\n",
    "    return colors_list\n",
    "\n",
    "\n",
    "def get_seg_overlay(image, seg):\n",
    "    \"\"\"\n",
    "    input: an image and a mask\n",
    "    output: the image and the mask combined, colorized by class\n",
    "    \"\"\"\n",
    "    color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)\n",
    "    palette = np.array(color_palette())\n",
    "    for label, color in enumerate(palette):\n",
    "        color_seg[seg == label, :] = color\n",
    "\n",
    "    # Show image + mask\n",
    "    img = np.array(image) * 0.8 + color_seg * 0.2\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "test_img_0 = get_seg_overlay(\n",
    "    test_ds_orig[1][\"pixel_values\"], np.array(test_ds_orig[1][\"label\"])\n",
    ")\n",
    "test_img_2 = get_seg_overlay(\n",
    "    test_ds_orig[2][\"pixel_values\"], np.array(test_ds_orig[2][\"label\"])\n",
    ")\n",
    "\n",
    "f, axs = plt.subplots(1, 2)\n",
    "\n",
    "axs[0].set_title(\"malignant\", {\"fontsize\": 12})\n",
    "axs[0].imshow(test_img_0)\n",
    "axs[1].set_title(\"benign\", {\"fontsize\": 12})\n",
    "axs[1].imshow(test_img_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentations and feature extraction (normalize, resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images for the transformer input\n",
    "# make sure data formats are right\n",
    "feature_extractor = SegformerFeatureExtractor(do_normalize=True)\n",
    "\n",
    "rotate_limit = 20\n",
    "shear_limit = 20\n",
    "\n",
    "# modeled loosely after the default FastAI augments\n",
    "training_augments = A.Compose(\n",
    "    [\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=0.9),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(\n",
    "            limit=(-rotate_limit, rotate_limit),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            border_mode=cv2.BORDER_REFLECT_101,\n",
    "            mask_value=0,\n",
    "            p=0.9,\n",
    "        ),\n",
    "        A.Affine(\n",
    "            scale=(0.833, 1.2),\n",
    "            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "            rotate=None,\n",
    "            shear={\"x\": (-shear_limit, shear_limit), \"y\": (-shear_limit, shear_limit)},\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            cval_mask=0,\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    \"\"\"\n",
    "    input: a batch of images and masks\n",
    "    output: augmented and preprocessed images and masks\n",
    "\n",
    "    This is for training.\n",
    "\n",
    "    Images get all the augmentations, including pixel-value (brightness, contrast).\n",
    "    Images also get feature-extracted (normalized).\n",
    "\n",
    "    Masks only get the geometric transforms (rotation, etc).\n",
    "    \"\"\"\n",
    "    batch_items = list(\n",
    "        zip(\n",
    "            [x for x in example_batch[\"pixel_values\"]],\n",
    "            [x for x in example_batch[\"label\"]],\n",
    "        )\n",
    "    )\n",
    "    batch_items_aug = [\n",
    "        training_augments(\n",
    "            image=np.array(x[0]),\n",
    "            mask=np.array(x[1]),\n",
    "        )\n",
    "        for x in batch_items\n",
    "    ]\n",
    "    images = [i[\"image\"] for i in batch_items_aug]\n",
    "    labels = [i[\"mask\"] for i in batch_items_aug]\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    \"\"\"\n",
    "    input: a batch of images and masks\n",
    "    output: augmented and preprocessed images and masks\n",
    "\n",
    "    This is for validation while training. No augmentations done here.\n",
    "\n",
    "    Images get normalized.\n",
    "    \"\"\"\n",
    "    images = [x for x in example_batch[\"pixel_values\"]]\n",
    "    labels = [x for x in example_batch[\"label\"]]\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Set transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show samples from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_img = 20\n",
    "\n",
    "for i in range(n_img):\n",
    "    train_item = train_ds[i]\n",
    "    if i == 0:\n",
    "        print(train_item[\"pixel_values\"].shape, train_item[\"labels\"].shape)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ax[0].imshow(train_item[\"pixel_values\"].T)\n",
    "    ax[1].imshow(train_item[\"labels\"].T)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained model, fine tune it on labeled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\n",
    "    # \"evaluate/metrics/mean_iou\",\n",
    "    \"mean_iou\",\n",
    "    num_process=multiprocessing.cpu_count(),\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute training / validation metrics.\n",
    "    This is called by the trainer.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "\n",
    "        # scale the logits to the size of the label\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric._compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_index=0,\n",
    "            reduce_labels=feature_extractor.reduce_labels,\n",
    "        )\n",
    "\n",
    "        # BUG: ignore_index=0 doesn't work properly\n",
    "        # mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
    "        #\n",
    "        # IoU for index=0 is always 0, but it's never ignored\n",
    "        # multiplying by 2 to fix it (2 metrics in the average, but the 2nd is always 0)\n",
    "        mean_iou = metrics[\"mean_iou\"]\n",
    "        metrics.update({\"mean_iou\": 2.0 * mean_iou})\n",
    "\n",
    "        # add per category metrics as individual key-value pairs\n",
    "        per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "        per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "\n",
    "        metrics.update(\n",
    "            {f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)}\n",
    "        )\n",
    "        metrics.update(\n",
    "            {f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)}\n",
    "        )\n",
    "\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    input: Optuna trial\n",
    "    output: best model performance metric\n",
    "\n",
    "    The objective function for hyperparameter optimization.\n",
    "    Used by study.optimize()\n",
    "    Calls train_model() with the appropriate hyperparameter values.\n",
    "    Captures and logs all output from train_model().\n",
    "    \"\"\"\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\n",
    "        \"lr_scheduler_type\", [\"linear\", \"cosine\"]\n",
    "    )\n",
    "    optim = trial.suggest_categorical(\"optimizer\", [\"adamw_hf\", \"adamw_torch\"])\n",
    "\n",
    "    # define log file, log parameters\n",
    "    trial_number_pad = str(trial.number).zfill(10)\n",
    "    output_file_name = trials_dir + \"/trial_\" + trial_number_pad\n",
    "    with open(output_file_name, \"w\") as cf:\n",
    "        print(f\"{trial.params}\\n\", file=cf)\n",
    "\n",
    "    # main training step\n",
    "    with capture_output(stdout=True, stderr=True, display=True) as captured:\n",
    "        best_metric = train_model(\n",
    "            learning_rate=learning_rate,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            optim=optim,\n",
    "            trial_number=trial.number,\n",
    "        )\n",
    "    with open(output_file_name, \"a\") as cf:\n",
    "        print(captured.stdout, file=cf)\n",
    "        print(captured.stderr, file=cf)\n",
    "\n",
    "    del captured\n",
    "    return best_metric\n",
    "\n",
    "\n",
    "def train_model(learning_rate, lr_scheduler_type, optim, trial_number):\n",
    "    \"\"\"\n",
    "    input: hyperparameter values and trial number\n",
    "    output: best IoU from model validation\n",
    "\n",
    "    Load a pretrained model.\n",
    "    Train it using the indicated hyperparameters.\n",
    "    Save the best model.\n",
    "    Return the best model performance.\n",
    "    \"\"\"\n",
    "\n",
    "    # delete/recreate folder with per-trial best and latest models\n",
    "    shutil.rmtree(output_dir, ignore_errors=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # fixed hyperparameters\n",
    "    # the model was chosen based on size\n",
    "    pretrained_model_name = \"nvidia/mit-b3\"\n",
    "    epochs = 20\n",
    "    batch_size = 8\n",
    "\n",
    "    # avoid printing useless warnings\n",
    "    logging.set_verbosity(50)\n",
    "    # load pretrained transformer from public repos\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        pretrained_model_name,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        num_labels=num_labels,\n",
    "    )\n",
    "    # restore normal verbosity\n",
    "    logging.set_verbosity(40)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=epochs,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        warmup_ratio=1e-2,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        auto_find_batch_size=False,\n",
    "        save_total_limit=3,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=20,\n",
    "        eval_steps=20,\n",
    "        logging_dir=logging_dir + \"/trial-\" + str(trial_number).zfill(10),\n",
    "        logging_steps=1,\n",
    "        log_level=\"error\",\n",
    "        eval_accumulation_steps=5,\n",
    "        disable_tqdm=False,\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"mean_iou\",\n",
    "        greater_is_better=True,\n",
    "        optim=optim,\n",
    "        push_to_hub=False,\n",
    "        dataloader_num_workers=multiprocessing.cpu_count(),\n",
    "        seed=20,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # extract best step data from trainer history\n",
    "    validation_history = []\n",
    "    for e in trainer.state.log_history:\n",
    "        if \"eval_mean_iou\" in e:\n",
    "            validation_history.append(e)\n",
    "    best_metric = 0\n",
    "    best_step = 0\n",
    "    for e in validation_history:\n",
    "        if e[\"eval_mean_iou\"] > best_metric:\n",
    "            best_metric = e[\"eval_mean_iou\"]\n",
    "            best_step = e[\"step\"]\n",
    "    print(\"\")\n",
    "    print(f\"best step / metric: {best_step} / {best_metric}\")\n",
    "\n",
    "    # move this trial's best model to model_dir\n",
    "    shutil.move(\n",
    "        output_dir + \"/checkpoint-\" + str(best_step),\n",
    "        model_dir + \"/trial-\" + str(trial_number).zfill(10),\n",
    "    )\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    return best_metric\n",
    "\n",
    "\n",
    "# study definition\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"train-all-semantic-mitb3-take2\",\n",
    "    storage=\"sqlite:///segformer_single_class_optuna.db\",\n",
    "    load_if_exists=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective, timeout=5 * 3600, show_progress_bar=True, gc_after_trial=True)\n",
    "# study.optimize(objective, n_trials=2, show_progress_bar=True, gc_after_trial=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show fine tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "print(f\"Best Trial: {best_trial.number}\")\n",
    "print(f\"Best Score: {best_trial.value}\")\n",
    "print(f\"Best Params:\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"\\t{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_contour(study)\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=800,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fig in [\n",
    "    optuna.visualization.plot_param_importances(study),\n",
    "    optuna.visualization.plot_optimization_history(study),\n",
    "    optuna.visualization.plot_slice(study),\n",
    "]:\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=1000,\n",
    "        height=360,\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload best trial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dir = model_dir + \"/trial-\" + str(best_trial.number).zfill(10)\n",
    "logging.set_verbosity(50)\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    best_model_dir, id2label=id2label, label2id=label2id, num_labels=num_labels\n",
    ")\n",
    "model.to(\"cuda:0\")\n",
    "logging.set_verbosity(40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample predictions on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_ind in tqdm(range(20, 40)):\n",
    "    image = test_ds[i_ind][\"pixel_values\"]\n",
    "    gt_seg = test_ds[i_ind][\"labels\"]\n",
    "\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    inputs.to(\"cuda:0\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # First, rescale logits to original image size\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.shape[1:],  # (height, width)\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    # Second, apply argmax on the class dimension\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "    image_orig = test_ds_orig[i_ind][\"pixel_values\"]\n",
    "    image_orig = image_orig.resize((512, 512), resample=PIL.Image.Resampling.BILINEAR)\n",
    "\n",
    "    pred_img = get_seg_overlay(image_orig, pred_seg.cpu())\n",
    "    gt_img = get_seg_overlay(image_orig, np.array(gt_seg))\n",
    "    f, axs = plt.subplots(1, 2)\n",
    "\n",
    "    axs[0].set_title(\"Prediction\", {\"fontsize\": 12})\n",
    "    axs[0].imshow(pred_img)\n",
    "    axs[1].set_title(\"Ground truth\", {\"fontsize\": 12})\n",
    "    axs[1].imshow(gt_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load video dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buv_df = buv_dataset_make(data_volume)\n",
    "# convert Path to string because HuggingFace needs the string type\n",
    "buv_df[\"image\"] = buv_df[\"image\"].apply(lambda x: str(x))\n",
    "buv_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_hf_video_ds(row):\n",
    "    ret = {}\n",
    "    ret[\"dataset\"] = row[\"dataset\"]\n",
    "    ret[\"class\"] = row[\"class\"]\n",
    "    ret[\"video\"] = row[\"video\"]\n",
    "    ret[\"str_index\"] = row[\"str_index\"]\n",
    "    # ret[\"pixel_values\"] = [Image.open(pv).convert(\"RGB\") for pv in row[\"pixel_values\"]]\n",
    "    ret[\"pixel_values\"] = Image.open(row[\"pixel_values\"]).convert(\"RGB\")\n",
    "    return ret\n",
    "\n",
    "\n",
    "def create_hf_video_dataset(df):\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"dataset\": df[\"dataset\"].to_list(),\n",
    "            \"class\": df[\"class\"].to_list(),\n",
    "            \"video\": df[\"video\"].to_list(),\n",
    "            \"pixel_values\": df[\"image\"].to_list(),\n",
    "            \"str_index\": df[\"str_index\"].to_list(),\n",
    "        }\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        map_hf_video_ds,\n",
    "        num_proc=multiprocessing.cpu_count(),\n",
    "        writer_batch_size=multiprocessing.cpu_count() ** 2,\n",
    "    )\n",
    "    dataset = dataset.cast_column(\n",
    "        \"dataset\", ClassLabel(names=df[\"dataset\"].unique().tolist())\n",
    "    )\n",
    "    dataset = dataset.cast_column(\n",
    "        \"class\", ClassLabel(names=df[\"class\"].unique().tolist())\n",
    "    )\n",
    "    dataset = dataset.cast_column(\n",
    "        \"video\", ClassLabel(names=df[\"video\"].unique().tolist())\n",
    "    )\n",
    "    dataset = dataset.cast_column(\"pixel_values\", ImageDS())\n",
    "    return dataset\n",
    "\n",
    "\n",
    "if os.path.isdir(video_dataset_path):\n",
    "    ds_hf_video = load_from_disk(video_dataset_path)\n",
    "else:\n",
    "    ds_hf_video = create_hf_video_dataset(buv_df)\n",
    "    ds_hf_video.save_to_disk(video_dataset_path)\n",
    "\n",
    "feature_extractor = SegformerFeatureExtractor()\n",
    "\n",
    "\n",
    "def video_transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"pixel_values\"]]\n",
    "    inputs = feature_extractor(images)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Not needed, feature_extractor() is applied below in the loop.\n",
    "# ds_hf_video.set_transform(video_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hf_video[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate mask predictions for video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "video_dataloader = DataLoader(\n",
    "    ds_hf_video,\n",
    "    batch_size=multiprocessing.cpu_count(),\n",
    "    num_workers=multiprocessing.cpu_count(),\n",
    ")\n",
    "\n",
    "from transformers import BatchFeature\n",
    "\"\"\"\n",
    "\n",
    "palette = np.array(color_palette())\n",
    "index_len = len(str(len(ds_hf_video)))\n",
    "\n",
    "outdir_base = \"frames\"\n",
    "outdir = outdir_base + \"/masks_segformer_single_class\"\n",
    "shutil.rmtree(outdir, ignore_errors=True)\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "for i, frame in tqdm(enumerate(ds_hf_video), total=len(ds_hf_video)):\n",
    "    inputs = feature_extractor(images=frame[\"pixel_values\"], return_tensors=\"pt\").to(\n",
    "        \"cuda:0\"\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits.cpu()\n",
    "    ups_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=(512, 512),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    predicted_segments = ups_logits.argmax(dim=1)[0]\n",
    "    mask = Image.fromarray(predicted_segments.numpy().astype(np.uint8), mode=\"L\")\n",
    "    image_index_str = str(i).rjust(index_len, \"0\")\n",
    "    mask.save(outdir + \"/f\" + image_index_str + \".png\", format=\"png\")\n",
    "\n",
    "    # color_seg = np.zeros((predicted_segments.shape[0], predicted_segments.shape[1], 3), dtype=np.uint8)\n",
    "    # for label, color in enumerate(palette):\n",
    "    # color_seg[predicted_segments == label, :] = color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render a few samples from the video dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = SegformerFeatureExtractor()\n",
    "\n",
    "bdl = buv_df.shape[0]\n",
    "# for i_ind in tqdm(range(buv_df.shape[0])):\n",
    "for i_ind in tqdm(list(range(5)) + list(range(bdl - 5, bdl))):\n",
    "    df_row = buv_df.loc[i_ind]\n",
    "    image = Image.open(df_row[\"image\"])\n",
    "    image_resize = image.resize((128, 128), resample=PIL.Image.Resampling.BILINEAR)\n",
    "\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    inputs.to(\"cuda:0\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # First, rescale logits to original image size\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image_resize.shape,  # (height, width)\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    # Second, apply argmax on the class dimension\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "    pred_img = get_seg_overlay(image_resize, pred_seg.cpu())\n",
    "\n",
    "    f, axs = plt.subplots(1, 2)\n",
    "\n",
    "    axs[0].set_title(\"Prediction\", {\"fontsize\": 12})\n",
    "    axs[0].imshow(pred_img)\n",
    "    axs[1].set_title(\"Original Image\", {\"fontsize\": 12})\n",
    "    axs[1].imshow(image_resize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
